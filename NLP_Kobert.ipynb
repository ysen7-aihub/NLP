{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Kobert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj3pgLhBdtMU"
      },
      "source": [
        "#필요 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk5KurEOKV2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5810cf-af53-427e-bfd0-19ec75ea31ea"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.45)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-2s19glzv\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-2s19glzv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2KNwyY8dx3x"
      },
      "source": [
        "#필요 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NakQGkaEdszU"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B61LSKN3d4hH"
      },
      "source": [
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5N7l6Syd_Bj"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhBS8VMbeBdH"
      },
      "source": [
        "##GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBeZoOgDeDGY",
        "outputId": "fe1af2fb-bff8-49a4-9141-34fb7e36df45"
      },
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Ngp3PHMwm_"
      },
      "source": [
        "#모델을 불러오기 위한 필수 클래스, 객체 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp2kh3xHf_-7"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lc_fYuhgCEz"
      },
      "source": [
        "## Setting parameters.\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gr1fV-igOpW"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmEEABN6JuaL"
      },
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKXk31FTJlWW"
      },
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay).\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9V6dlsgUnb"
      },
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNsDdelahfmm"
      },
      "source": [
        "#모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix3Rf15ohh4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d94abb-5086-4e36-e4f4-8d9a3c1bae1e"
      },
      "source": [
        "#구글드라이브 연동 -->추후 깃허브에 모델 올릴건데 그걸로 서버 연동해서 하면 될 것 같습니다.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxwwDRtYhjkc"
      },
      "source": [
        "#★★★현재경로가 model이 있는 폴더여야함★★★ -->여기도 깃허브에 모델 올릴테니 서버에 연동하면 될겁니다.\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/파이썬공부/챗봇/models/')\n",
        "\n",
        "model1 = torch.load('7emotions_model.pt')                                       # 전체 모델을 통째로 불러옴, 클래스 선언 필수.\n",
        "model1.load_state_dict(torch.load('7emotions_model_state_dict.pt'))             # state_dict를 불러 온 후, 모델에 저장.\n",
        "\n",
        "checkpoint = torch.load('7emotions_all.tar')                                    # dict 불러오기.\n",
        "model1.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqReyQHliGtT"
      },
      "source": [
        "#불러온 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXrJce8hoNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a346d83-0cfc-4822-a8bc-60b0b6b6b194"
      },
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x0DlJqCiND8"
      },
      "source": [
        "def split_text(non_splited_text):\n",
        "  splited_text = non_splited_text.split('.')                                    # .(마침표)단위로 끊어낸 뒤 자동으로 리스트 형 변환.\n",
        "  return splited_text\n",
        "\n",
        "\n",
        "def predict(predict_sentence):                                                  #참고 : predict_sentence는 \\n 없이 입력받아야 합니다.\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model1.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model1(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                eval = '공포'\n",
        "            elif np.argmax(logits) == 1:\n",
        "                eval = '놀람'\n",
        "            elif np.argmax(logits) == 2:\n",
        "                eval = '분노'\n",
        "            elif np.argmax(logits) == 3:\n",
        "                eval = '슬픔'\n",
        "            elif np.argmax(logits) == 4:\n",
        "                eval = '중립'\n",
        "            elif np.argmax(logits) == 5:\n",
        "                eval = '행복'\n",
        "            elif np.argmax(logits) == 6:\n",
        "                eval = '혐오'\n",
        "        return eval\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEQqz2xGw_cZ"
      },
      "source": [
        "def SolveTheResult(text):                                                       #들어가는 text는 str형이며, \\n이 없도록 주의해주세요.\n",
        "  emotions = {'공포':0,'놀람':0,'분노':0,'슬픔':0,'중립':0,'행복':0,'혐오':0}   #초기값은 모두 0.\n",
        "  text = text.split('.')                                                        #마침표 단위로 문장을 끊습니다.\n",
        "  for splited_text in text:\n",
        "    if predict(splited_text)=='공포':\n",
        "      emotions['공포']+=1\n",
        "    elif predict(splited_text)=='놀람':\n",
        "      emotions['놀람']+=1\n",
        "    elif predict(splited_text)=='분노':\n",
        "      emotions['분노']+=1\n",
        "    elif predict(splited_text)=='슬픔':\n",
        "      emotions['슬픔']+=1\n",
        "    elif predict(splited_text)=='중립':\n",
        "      emotions['중립']+=1\n",
        "    elif predict(splited_text)=='행복':\n",
        "      emotions['행복']+=1\n",
        "    elif predict(splited_text)=='혐오':\n",
        "      emotions['혐오']+=1\n",
        "\n",
        "  emotions = sorted(emotions.items(), key = lambda item: item[1], reverse=True) #emotions를 오름차순으로 정렬\n",
        "                                                                                #list 내부에 tuple이 있는 형태가 됨 --> tuple을 바꿀 수 없으므로 내부까지 list로 변환.\n",
        "  emotions = [list(emotions[x]) for x in range(len(emotions))]                  #내부까지 list형으로 변환.\n",
        "\n",
        "  summary = sum(int(j) for i, j in emotions)                                    #emotions의 모든 감정의 개수\n",
        "  \n",
        "  for i in range(len(emotions)):                                                #emotions의 각각의 감정을 총합과 나누어 %로 나타낸다.\n",
        "    try:                                                                        #감정의 값이 0일 경우를 위한 예외처리\n",
        "      emotions[i][1] = emotions[i][1] / summary\n",
        "    except ZeroDivisionError:\n",
        "      pass\n",
        "  emotions = emotions[0:3]                                                      #emotions의 상위 3개값만 출력\n",
        "  return emotions"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2XsuuestYYc"
      },
      "source": [
        "#테스트를 위한 문장이므로 실제 사용시에는 지워주세요.\n",
        "test_text = '수요일은 특식 데이이다. 학교 급식에서 잔반 없는 날 이라고 하여 맛있는 급식을 주는 날이다. 바로 오늘이 수요일이라 아침부터 기대가 되었다. 오늘의 반찬은 치킨 샐러드와 스파게티, 옥수수 수프 그리고 고구마튀김이 나와서 친구들 모두 좋아했다. 그중에서 스파게티는 내 짝 정연이가 좋아하는 음식이다. 정연이는 돈가스와 스파게티를 제일 좋아한다. 내가 제일 좋아하는 만두는 없어서 아쉬웠지만, 정연이가 만두가 맛있는 분식집을 알려주어서 아쉬운 마음이 사라졌다. 주말에 엄마 아빠에게 그 분식집에 가보자고 해야지.'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3jDvTRtHPcw",
        "outputId": "0f4a2ff3-cbe9-4119-d1f2-04946c2fb72c"
      },
      "source": [
        "print(SolveTheResult(test_text))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['행복', 0.6666666666666666], ['중립', 0.3333333333333333], ['공포', 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}